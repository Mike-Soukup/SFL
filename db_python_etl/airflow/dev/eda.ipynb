{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Overview:\n",
    "\n",
    "The intent of this notebook is to conduct initial Exploratory Data Analysis (EDA) on the Dataset provided: `SRDataEngineerChallenge_DATASET.csv`. This will allow for data insights to ensure a proper ETL pipeline can be produced for this dataset. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in Pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../SRDataEngineerChallenge_DATASET.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 6 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   id          1000 non-null   int64 \n",
      " 1   first_name  1000 non-null   object\n",
      " 2   last_name   1000 non-null   object\n",
      " 3   email       1000 non-null   object\n",
      " 4   gender      1000 non-null   object\n",
      " 5   ip_address  1000 non-null   object\n",
      "dtypes: int64(1), object(5)\n",
      "memory usage: 47.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data set is small at only 47.0 KB. This is nice because it means we can work with it simply using Pandas and processing will not require any distributed compute. \n",
    "\n",
    "Additionally, data does not contain any null values. This means we do not have to determine how to handle nulls via deletion or imputation. \n",
    "\n",
    "Finally, we learn here that the data is largely text based. Outside of the initial index provided with the data, all of the data types are objects indicating non-numerical or string type values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the head of data a couple things stand out:\n",
    "\n",
    "1. This is PII Data. Every column besides the `id` column can be considered a PII field. As a result, we must consider data privacy. As a result, a good transformation idea for our pipeline will be to scrub human PII via a hasing algorithm such as `MD5`. \n",
    "2. It appears as though gender values fall into discrete buckets. As a result, it may be best to map the data in that column to smaller values while still retaining the information they are holding. I.e. Male -> M\n",
    "3. A relational data store appears to be the most obvious persistent storage solution for this data. The data is already structured with a well defined schema. \n",
    "4. If we intend to place this data into a relational data store, it will be important to consider which field(s) will make up the primary and foreign keys. A primary key must consist of a unique identifier for each column. Also, because the intent of this data is unkown at this time, it is difficult to make a firm decision on schema and keys as well. For now, let's assume that we want the Primary Key to be a column that we can share external to this datastore to link back to each individual's data. As a result, a hash of the `first_name`, `last_name`, `email`, and `ip_address` will ensure a single Primary Key for this table and will ensure that primary key can be used without worrying about displaying PII. \n",
    "5. ** For now, we will store all data fields until a better understanding of data privacy and regulation requirements for this data is understood.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Genderfluid    140\n",
       "Female         131\n",
       "Genderqueer    130\n",
       "Male           129\n",
       "Agender        120\n",
       "Bigender       119\n",
       "Polygender     118\n",
       "Non-binary     113\n",
       "Name: gender, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['gender'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a relatively equitabble distribution of discrete gender fields. This makes the gender field a good candidate for transformation via mapping to enable more efficient data storage in our relational database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENDER_MAP = {\n",
    "    'Genderfluid':'GF',\n",
    "    'Female':'F',\n",
    "    'Genderqueer':'GQ',\n",
    "    'Male':'M',\n",
    "    'Agender':'A',\n",
    "    'Bigender':'B',\n",
    "    'Polygender':'P',\n",
    "    'Non-binary':'NB'\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing we want to note at this stage is that we may not know all of the potential gender field options at this time nor do we know how standard the inputs we receive will be. I.e. Will the first letter always be capitalized? Will non-binary always be hyponated? At this time, we will apply a default option if we don't receive an exact string match when we map our gender variables. But this is something to note and consider for future development for a more robust and scalable ETL Pipeline for generating more useful data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Map gender values and provide 'UNKWN' string if input gender value is Unknown:\n",
    "df['gender'] = df['gender'].apply(lambda x: GENDER_MAP.get(x, 'UNKWN'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GF    140\n",
       "F     131\n",
       "GQ    130\n",
       "M     129\n",
       "A     120\n",
       "B     119\n",
       "P     118\n",
       "NB    113\n",
       "Name: gender, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['gender'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's execute the hashing of our PII to create our Primary Key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "df['concat'] = df['first_name'] + df['last_name'] + df['email'] + df['ip_address']\n",
    "df['hash'] = df['concat'].apply(lambda x: hashlib.md5(x.encode()).hexdigest())\n",
    "df.drop(columns = ['concat'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['id','hash','first_name','last_name','email','gender','ip_address']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion:\n",
    "\n",
    "For the Airflow portion of this solution, the Python ETL Pipeline will conduct the following actions:\n",
    "\n",
    "1. Read a flat csv file into a Pandas DataFrame\n",
    "2. Map gender fields and create a PII sensitve primary key\n",
    "3. Ingest the dataframe into a PostgreSQL Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f7cb2ffcb444c1057d4801dca1573466b9dc128abcb87b9f586e955c5901ca6d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
